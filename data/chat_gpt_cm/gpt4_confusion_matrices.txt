Model: gpt4, Essay Set: 5, score 1 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|915|303|125|048 | TP: 915, 65.78%; FN: 476; Total: 1391
1|022|087|110|109 | TP: 87, 26.52%; FN: 241; Total: 328
2|000|000|006|036 | TP: 6, 14.29%; FN: 36; Total: 42
3|000|000|000|034 | TP: 34, 100.0%; FN: 0; Total: 34
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 915 & 303 & 125 & 048 \\
    \hline
    1 & 022 & 087 & 110 & 109 \\
    \hline
    2 & 000 & 000 & 006 & 036 \\
    \hline
    3 & 000 & 000 & 000 & 034 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9765208110992529
Recall: 0.6578001437814522
F1: 0.7860824742268041
-------------------------------------
Category: 1
Precision: 0.2230769230769231
Recall: 0.2652439024390244
F1: 0.2423398328690808
-------------------------------------
Category: 2
Precision: 0.024896265560165973
Recall: 0.14285714285714285
F1: 0.04240282685512367
-------------------------------------
Category: 3
Precision: 0.14977973568281938
Recall: 1.0
F1: 0.26053639846743293
-------------------------------------
Weighted across categories
Accuracy: 0.5805013927576602
Precision: 0.8009180129108827
Recall: 0.5805013927576602
F1: 0.6593689933740116
###################################################

Model: gpt4, Essay Set: 5, score 1 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|913|308|121|049 | TP: 913, 65.64%; FN: 478; Total: 1391
1|024|082|114|101 | TP: 82, 25.55%; FN: 239; Total: 321
2|000|000|006|042 | TP: 6, 12.5%; FN: 42; Total: 48
3|000|000|000|035 | TP: 35, 100.0%; FN: 0; Total: 35
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 913 & 308 & 121 & 049 \\
    \hline
    1 & 024 & 082 & 114 & 101 \\
    \hline
    2 & 000 & 000 & 006 & 042 \\
    \hline
    3 & 000 & 000 & 000 & 035 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9743863393810032
Recall: 0.6563623292595255
F1: 0.7843642611683849
-------------------------------------
Category: 1
Precision: 0.21025641025641026
Recall: 0.2554517133956386
F1: 0.23066104078762306
-------------------------------------
Category: 2
Precision: 0.024896265560165973
Recall: 0.125
F1: 0.04152249134948097
-------------------------------------
Category: 3
Precision: 0.15418502202643172
Recall: 1.0
F1: 0.26717557251908397
-------------------------------------
Weighted across categories
Accuracy: 0.577158774373259
Precision: 0.7963538731415578
Recall: 0.577158774373259
F1: 0.6553967164350939
###################################################

Model: gpt4, Essay Set: 5, score 2 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|917|301|127|046 | TP: 917, 65.92%; FN: 474; Total: 1391
1|022|084|113|109 | TP: 84, 25.61%; FN: 244; Total: 328
2|000|001|006|035 | TP: 6, 14.29%; FN: 36; Total: 42
3|000|000|000|034 | TP: 34, 100.0%; FN: 0; Total: 34
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 917 & 301 & 127 & 046 \\
    \hline
    1 & 022 & 084 & 113 & 109 \\
    \hline
    2 & 000 & 001 & 006 & 035 \\
    \hline
    3 & 000 & 000 & 000 & 034 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9765708200212992
Recall: 0.6592379583033788
F1: 0.7871244635193133
-------------------------------------
Category: 1
Precision: 0.21761658031088082
Recall: 0.25609756097560976
F1: 0.23529411764705882
-------------------------------------
Category: 2
Precision: 0.024390243902439025
Recall: 0.14285714285714285
F1: 0.04166666666666667
-------------------------------------
Category: 3
Precision: 0.15178571428571427
Recall: 1.0
F1: 0.26356589147286824
-------------------------------------
Weighted across categories
Accuracy: 0.5799442896935934
Precision: 0.7999851551650211
Recall: 0.5799442896935934
F1: 0.6589291585814361
###################################################

Model: gpt4, Essay Set: 5, score 2 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|917|301|126|047 | TP: 917, 65.92%; FN: 474; Total: 1391
1|022|085|113|101 | TP: 85, 26.48%; FN: 236; Total: 321
2|000|000|006|042 | TP: 6, 12.5%; FN: 42; Total: 48
3|000|000|001|034 | TP: 34, 97.14%; FN: 1; Total: 35
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 917 & 301 & 126 & 047 \\
    \hline
    1 & 022 & 085 & 113 & 101 \\
    \hline
    2 & 000 & 000 & 006 & 042 \\
    \hline
    3 & 000 & 000 & 001 & 034 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9765708200212992
Recall: 0.6592379583033788
F1: 0.7871244635193133
-------------------------------------
Category: 1
Precision: 0.22020725388601037
Recall: 0.26479750778816197
F1: 0.24045261669024043
-------------------------------------
Category: 2
Precision: 0.024390243902439025
Recall: 0.125
F1: 0.04081632653061225
-------------------------------------
Category: 3
Precision: 0.15178571428571427
Recall: 0.9714285714285714
F1: 0.2625482625482625
-------------------------------------
Weighted across categories
Accuracy: 0.5805013927576602
Precision: 0.7997658890553504
Recall: 0.5805013927576602
F1: 0.6591775997635603
###################################################

Model: gpt4, Essay Set: 6, score 1 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1057|276|099|083 | TP: 1057, 69.77%; FN: 458; Total: 1515
1|001|045|057|057 | TP: 45, 28.12%; FN: 115; Total: 160
2|000|000|017|054 | TP: 17, 23.94%; FN: 54; Total: 71
3|000|000|000|051 | TP: 51, 100.0%; FN: 0; Total: 51
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1057 & 276 & 099 & 083 \\
    \hline
    1 & 001 & 045 & 057 & 057 \\
    \hline
    2 & 000 & 000 & 017 & 054 \\
    \hline
    3 & 000 & 000 & 000 & 051 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.999054820415879
Recall: 0.6976897689768977
F1: 0.8216090167120095
-------------------------------------
Category: 1
Precision: 0.14018691588785046
Recall: 0.28125
F1: 0.18711018711018707
-------------------------------------
Category: 2
Precision: 0.09826589595375723
Recall: 0.23943661971830985
F1: 0.13934426229508196
-------------------------------------
Category: 3
Precision: 0.20816326530612245
Recall: 1.0
F1: 0.34459459459459457
-------------------------------------
Weighted across categories
Accuracy: 0.6510851419031719
Precision: 0.8645471144215033
Recall: 0.6510851419031719
F1: 0.7246205104082356
###################################################

Model: gpt4, Essay Set: 6, score 1 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1057|277|098|082 | TP: 1057, 69.82%; FN: 457; Total: 1514
1|001|044|059|057 | TP: 44, 27.33%; FN: 117; Total: 161
2|000|000|016|055 | TP: 16, 22.54%; FN: 55; Total: 71
3|000|000|000|051 | TP: 51, 100.0%; FN: 0; Total: 51
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1057 & 277 & 098 & 082 \\
    \hline
    1 & 001 & 044 & 059 & 057 \\
    \hline
    2 & 000 & 000 & 016 & 055 \\
    \hline
    3 & 000 & 000 & 000 & 051 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.999054820415879
Recall: 0.6981505944517834
F1: 0.8219284603421463
-------------------------------------
Category: 1
Precision: 0.13707165109034267
Recall: 0.2732919254658385
F1: 0.18257261410788383
-------------------------------------
Category: 2
Precision: 0.09248554913294797
Recall: 0.22535211267605634
F1: 0.13114754098360656
-------------------------------------
Category: 3
Precision: 0.20816326530612245
Recall: 1.0
F1: 0.34459459459459457
-------------------------------------
Weighted across categories
Accuracy: 0.6499721758486366
Precision: 0.8635616774926198
Recall: 0.6499721758486366
F1: 0.7238061655890591
###################################################

Model: gpt4, Essay Set: 6, score 2 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1058|280|093|084 | TP: 1058, 69.83%; FN: 457; Total: 1515
1|000|045|060|055 | TP: 45, 28.12%; FN: 115; Total: 160
2|000|000|017|054 | TP: 17, 23.94%; FN: 54; Total: 71
3|000|000|000|051 | TP: 51, 100.0%; FN: 0; Total: 51
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1058 & 280 & 093 & 084 \\
    \hline
    1 & 000 & 045 & 060 & 055 \\
    \hline
    2 & 000 & 000 & 017 & 054 \\
    \hline
    3 & 000 & 000 & 000 & 051 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 1.0
Recall: 0.6983498349834983
F1: 0.8223863194714341
-------------------------------------
Category: 1
Precision: 0.13846153846153847
Recall: 0.28125
F1: 0.18556701030927836
-------------------------------------
Category: 2
Precision: 0.1
Recall: 0.23943661971830985
F1: 0.14107883817427386
-------------------------------------
Category: 3
Precision: 0.20901639344262296
Recall: 1.0
F1: 0.34576271186440677
-------------------------------------
Weighted across categories
Accuracy: 0.6516416249304396
Precision: 0.86528307302138
Recall: 0.6516416249304396
F1: 0.725240117676219
###################################################

Model: gpt4, Essay Set: 6, score 2 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1058|280|094|082 | TP: 1058, 69.88%; FN: 456; Total: 1514
1|000|045|060|056 | TP: 45, 27.95%; FN: 116; Total: 161
2|000|000|016|055 | TP: 16, 22.54%; FN: 55; Total: 71
3|000|000|000|051 | TP: 51, 100.0%; FN: 0; Total: 51
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1058 & 280 & 094 & 082 \\
    \hline
    1 & 000 & 045 & 060 & 056 \\
    \hline
    2 & 000 & 000 & 016 & 055 \\
    \hline
    3 & 000 & 000 & 000 & 051 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 1.0
Recall: 0.6988110964332893
F1: 0.822706065318818
-------------------------------------
Category: 1
Precision: 0.13846153846153847
Recall: 0.2795031055900621
F1: 0.1851851851851852
-------------------------------------
Category: 2
Precision: 0.09411764705882353
Recall: 0.22535211267605634
F1: 0.13278008298755187
-------------------------------------
Category: 3
Precision: 0.20901639344262296
Recall: 1.0
F1: 0.34576271186440677
-------------------------------------
Weighted across categories
Accuracy: 0.6510851419031719
Precision: 0.8645712279905721
Recall: 0.6510851419031719
F1: 0.7247930338924353
###################################################

Model: gpt4, score 1 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1972|579|224|131 | TP: 1972, 67.86%; FN: 934; Total: 2906
1|023|132|167|166 | TP: 132, 27.05%; FN: 356; Total: 488
2|000|000|023|090 | TP: 23, 20.35%; FN: 90; Total: 113
3|000|000|000|085 | TP: 85, 100.0%; FN: 0; Total: 85
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1972 & 579 & 224 & 131 \\
    \hline
    1 & 023 & 132 & 167 & 166 \\
    \hline
    2 & 000 & 000 & 023 & 090 \\
    \hline
    3 & 000 & 000 & 000 & 085 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9884711779448622
Recall: 0.678596008258775
F1: 0.8047337278106508
-------------------------------------
Category: 1
Precision: 0.18565400843881857
Recall: 0.27049180327868855
F1: 0.22018348623853212
-------------------------------------
Category: 2
Precision: 0.05555555555555555
Recall: 0.20353982300884957
F1: 0.0872865275142315
-------------------------------------
Category: 3
Precision: 0.18008474576271186
Recall: 1.0
F1: 0.30520646319569117
-------------------------------------
Weighted across categories
Accuracy: 0.6158129175946548
Precision: 0.8309246604659024
Recall: 0.6158129175946548
F1: 0.6909275281967976
###################################################

Model: gpt4, score 1 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1970|585|219|131 | TP: 1970, 67.81%; FN: 935; Total: 2905
1|025|126|173|158 | TP: 126, 26.14%; FN: 356; Total: 482
2|000|000|022|097 | TP: 22, 18.49%; FN: 97; Total: 119
3|000|000|000|086 | TP: 86, 100.0%; FN: 0; Total: 86
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1970 & 585 & 219 & 131 \\
    \hline
    1 & 025 & 126 & 173 & 158 \\
    \hline
    2 & 000 & 000 & 022 & 097 \\
    \hline
    3 & 000 & 000 & 000 & 086 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9874686716791979
Recall: 0.6781411359724613
F1: 0.8040816326530612
-------------------------------------
Category: 1
Precision: 0.17721518987341772
Recall: 0.26141078838174275
F1: 0.21123218776194466
-------------------------------------
Category: 2
Precision: 0.05314009661835749
Recall: 0.18487394957983194
F1: 0.0825515947467167
-------------------------------------
Category: 3
Precision: 0.18220338983050846
Recall: 1.0
F1: 0.3082437275985663
-------------------------------------
Weighted across categories
Accuracy: 0.6135857461024499
Precision: 0.8285098484883258
Recall: 0.6135857461024499
F1: 0.6887538022568863
###################################################

Model: gpt4, score 2 vs experts score 1
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1975|581|220|130 | TP: 1975, 67.96%; FN: 931; Total: 2906
1|022|129|173|164 | TP: 129, 26.43%; FN: 359; Total: 488
2|000|001|023|089 | TP: 23, 20.35%; FN: 90; Total: 113
3|000|000|000|085 | TP: 85, 100.0%; FN: 0; Total: 85
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1975 & 581 & 220 & 130 \\
    \hline
    1 & 022 & 129 & 173 & 164 \\
    \hline
    2 & 000 & 001 & 023 & 089 \\
    \hline
    3 & 000 & 000 & 000 & 085 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9889834752128193
Recall: 0.6796283551273228
F1: 0.8056292066081991
-------------------------------------
Category: 1
Precision: 0.18143459915611815
Recall: 0.26434426229508196
F1: 0.2151793160967473
-------------------------------------
Category: 2
Precision: 0.055288461538461536
Recall: 0.20353982300884957
F1: 0.08695652173913043
-------------------------------------
Category: 3
Precision: 0.18162393162393162
Recall: 1.0
F1: 0.3074141048824593
-------------------------------------
Weighted across categories
Accuracy: 0.6158129175946548
Precision: 0.8307939013637302
Recall: 0.6158129175946548
F1: 0.6910139940228759
###################################################

Model: gpt4, score 2 vs experts score 2
-------------------------------------
 | 0 | 1 | 2 | 3
-------------------
0|1975|581|220|129 | TP: 1975, 67.99%; FN: 930; Total: 2905
1|022|130|173|157 | TP: 130, 26.97%; FN: 352; Total: 482
2|000|000|022|097 | TP: 22, 18.49%; FN: 97; Total: 119
3|000|000|001|085 | TP: 85, 98.84%; FN: 1; Total: 86
-------------------------------------
LaTeX:
\begin{tabular}{|c|c|c|c|c|}
    \hline
        & 0 & 1 & 2 & 3 \\
    \hline
    0 & 1975 & 581 & 220 & 129 \\
    \hline
    1 & 022 & 130 & 173 & 157 \\
    \hline
    2 & 000 & 000 & 022 & 097 \\
    \hline
    3 & 000 & 000 & 001 & 085 \\
    \hline
\end{tabular}
-------------------------------------
Category: 0
Precision: 0.9889834752128193
Recall: 0.6798623063683304
F1: 0.8057935536515708
-------------------------------------
Category: 1
Precision: 0.1828410689170183
Recall: 0.2697095435684647
F1: 0.21793797150041913
-------------------------------------
Category: 2
Precision: 0.052884615384615384
Recall: 0.18487394957983194
F1: 0.08224299065420561
-------------------------------------
Category: 3
Precision: 0.18162393162393162
Recall: 0.9883720930232558
F1: 0.30685920577617326
-------------------------------------
Weighted across categories
Accuracy: 0.6158129175946548
Precision: 0.8304675161641619
Recall: 0.6158129175946548
F1: 0.6909947614714969
###################################################


from config import Configuration
from tsv_utils import get_ratings, Rating
from typing import List, Tuple

from sklearn.metrics import cohen_kappa_score
import matplotlib.pyplot as plt
import numpy as np


def _mock_kappa(samples):
    rating_a = np.random.randint(1, 6, samples).tolist()
    rating_b = np.random.randint(1, 6, samples).tolist()
    return cohen_kappa_score(rating_a, rating_b, weights='quadratic')

# TODO: remove
def _mock_plot():
    samples = range(1, 100)
    kappas1 = [_mock_kappa(sample) for sample in samples]
    kappas2 = [_mock_kappa(sample) for sample in samples]
    kappas3 = [_mock_kappa(sample) for sample in samples]

    plt.plot(samples, kappas1, color='r')
    plt.plot(samples, kappas2, color='g')
    plt.plot(samples, kappas3, color='b')

# Score 1 is the rating, generated by our trained model
def _get_scores(ratings: List[Rating]) -> List[int]:
    return [rating.answer.score_1 for rating in ratings]

def _filter_for_id(ratings:List[Rating], ids: List[str]):
    return [rating for rating in ratings if rating.answer.answer_id in ids]

# for the result to be comparable, both lists need to consist of the same answers
def _remove_unique_answers(ratings_a:List[Rating], ratings_b:List[Rating]) -> Tuple[List[Rating], List[Rating]]:
    rating_ids_a = set([rating.answer.answer_id for rating in ratings_a])
    rating_ids_b = set([rating.answer.answer_id for rating in ratings_b])

    common_ids = rating_ids_a.intersection(rating_ids_b)

    return _filter_for_id(ratings_a, common_ids), _filter_for_id(ratings_b, common_ids)

def evaluate_ratings(questions_path, fileA, fileB):
	# load and sort ratings, if the answer_ids do not match, the answers do not match
	# this would invalidate the comparison
    ratings_a = sorted(get_ratings(fileA, questions_path), key=lambda rating: rating.answer.answer_id)
    ratings_b = sorted(get_ratings(fileB, questions_path), key=lambda rating: rating.answer.answer_id)
    
    ratings_a, ratings_b = _remove_unique_answers(ratings_a, ratings_b)
    
    scores_a = _get_scores(ratings_a)
    scores_b = _get_scores(ratings_b)

    kappa = cohen_kappa_score(scores_a, scores_b, weights='quadratic')
    plt.scatter(len(scores_a), kappa, color='r')

def test():
    config = Configuration()
    questions_path = config.get_questions_path()

    # TODO: parameterize, the two rated answer-sets to be compared
    evaluate_ratings(questions_path, config.get_rated_answers_path('01'), config.get_rated_answers_path('02'))


test()

# Set the labels for the x and y axis
plt.xlabel('Amount of samples')
plt.ylabel('Kappa')

# Save the figure to a pdf file
plt.savefig('diagram.pdf')

# Display the plot
plt.show()

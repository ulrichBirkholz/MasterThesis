from config import Configuration
from tsv_utils import get_ratings, Rating
from typing import List, Tuple, Dict
from tsv_utils import get_answers
import logging as log

from sklearn.metrics import cohen_kappa_score
import matplotlib.pyplot as plt

class KappaFigure():
    def __init__(self, title:str, identifier:str) -> None:
        self.figure = plt.figure()
        self.answer_count = -1
        self.all_counts_equal = True
        plt.title(title)
        plt.xlabel('Amount of samples the model was trained with')
        plt.ylabel('Kappa')

        self.filename = f"kappa_{identifier}.pdf"
    
    def plot(self, x, y, answer_count):
        if y < -1 or y > 1:
            log.warning(f"Ignore invalid kappa: {y}")
            return

        plt.figure(self.figure.number)
        plt.plot(x, y, 'ro')

        if self.answer_count == -1:
            self.answer_count = answer_count
        elif self.answer_count != answer_count:
            # should not happen the data might be inconsistent
            self.all_counts_equal = False
            log.error(f"Data is potentially inconsistent")

            # find the smallest value
            if self.answer_count > answer_count:
                self.answer_count = answer_count

    def save(self, config: Configuration, figtext:str=None):
        plt.figure(self.figure.number)
        if figtext is not None:
            plt.figtext(figtext)
        elif self.all_counts_equal is True:
            plt.figtext(0.5, 1, f"Each value represents the agreement of exactly {self.answer_count} ratings", fontsize=12, bbox={'facecolor': 'grey', 'alpha': 0.5, 'pad': 10})
        else:
            plt.figtext(0.5, 1, f"Each value represents the agreement of at leased {self.answer_count} ratings", fontsize=12, bbox={'facecolor': 'grey', 'alpha': 0.5, 'pad': 10})

        log.debug(f"Save Diagram: {self.filename}")
        self.figure.savefig(config.get_path_for_datafile(self.filename), bbox_inches='tight')

# Score 1 is the rating, generated by our trained model
def _get_scores_1(ratings: List[Rating]) -> List[int]:
    return [rating.answer.score_1 for rating in ratings]

# Score 2 is the rating, generated by ChatGPT
def _get_scores_2(ratings: List[Rating]) -> List[int]:
    return [rating.answer.score_2 for rating in ratings]

def _filter_for_id(ratings:List[Rating], ids: List[str]):
    return [rating for rating in ratings if rating.answer.answer_id in ids]

# for the result to be comparable, both lists need to consist of the same answers
def _remove_unique_answers(ratings_a:List[Rating], ratings_b:List[Rating]) -> Tuple[List[Rating], List[Rating]]:
    rating_ids_a = set([rating.answer.answer_id for rating in ratings_a])
    rating_ids_b = set([rating.answer.answer_id for rating in ratings_b])

    common_ids = rating_ids_a.intersection(rating_ids_b)

    return _filter_for_id(ratings_a, common_ids), _filter_for_id(ratings_b, common_ids)

def _calculate_kappa(scores_a, scores_b):
    if len(scores_a) != len(scores_b) or len(scores_a) < 100:
        log.warning(f"Not enough scores: {len(scores_a)} to calculate kappa")
        return -5, len(scores_a)
    return cohen_kappa_score(scores_a, scores_b, weights='quadratic'), len(scores_a)

def _calculate_kappa_score(own_rating, external_rating):
    # load and sort ratings, if the answer_ids do not match, the answers do not match
    # this would invalidate the comparison
    ratings_a = sorted(own_rating, key=lambda rating: rating.answer.answer_id)
    ratings_b = sorted(external_rating, key=lambda rating: rating.answer.answer_id)

    # we can only evaluate matching answer sets
    ratings_a, ratings_b = _remove_unique_answers(ratings_a, ratings_b)
    
    scores_a = _get_scores_1(ratings_a)
    scores_b = _get_scores_1(ratings_b)

    return _calculate_kappa(scores_a, scores_b)

def _answers_to_rating(answers):
     return [Rating(None, answer) for answer in answers]

def _get_figure_for_dataset(diagrams:Dict, id:str, model_descriptor:str, answer_descriptor:str) -> KappaFigure:
    identifier = f"{model_descriptor}_model_{answer_descriptor}_answers_{id}"
    platform = model_descriptor.split("_")[0]
    
    if identifier not in diagrams:
        if model_descriptor.endswith("ai"):
            diagram_title = f"{platform} AI-Refined"
            if answer_descriptor == "ai-training":
                diagram_title += " ML Model\nrating answers used for training"
            elif answer_descriptor == "man-rating":
                diagram_title += " ML Model\nrating manually created answers"
            elif answer_descriptor == "ai-rating":
                diagram_title += " ML Model"
            else:
                log.warning(f"The answer_descriptor {answer_descriptor} is invalid")
                return

        elif model_descriptor.endswith("man"):
            diagram_title = f"{platform} Expert-Refined"
            if answer_descriptor == "man-training":
                diagram_title += " ML Model\nrating answers used for training"
            elif answer_descriptor == "ai-rating":
                diagram_title += " ML Model\nrating answers created by AI"
            elif answer_descriptor == "man-rating":
                diagram_title += " ML Model"
            else:
                log.warning(f"The answer_descriptor {answer_descriptor} is invalid")
                return

        elif model_descriptor.endswith("ai-v-man"):
            diagram_title = f"{platform} AI-Refined compared with Expert-Refined ML Model\nrating "
            if answer_descriptor == "ai-rating":
                diagram_title += "answers created by AI"
            
            elif answer_descriptor == "man-rating":
                diagram_title += "manually created answers"

            else:
                log.warning(f"The answer_descriptor {answer_descriptor} is invalid")
                return

        else:
            log.warning(f"The model_descriptor {model_descriptor} is invalid")
            return

        diagrams[identifier] = KappaFigure(diagram_title, identifier)
    
    return diagrams[identifier]


if __name__ == "__main__":
    log.basicConfig(level=log.WARNING)
    config = Configuration()
    questions_path = config.get_questions_path()

    all_ai_answers = _answers_to_rating(get_answers(config.get_ai_answers_path()))
    all_man_answers = _answers_to_rating(get_answers(config.get_man_answers_path()))

    ai_answers_rating = _answers_to_rating(get_answers(config.get_ai_answers_to_rate_path()))
    man_answers_rating = _answers_to_rating(get_answers(config.get_man_answers_to_rate_path()))

    ai_answers_training = _answers_to_rating(get_answers(config.get_ai_answers_for_training_path()))
    man_answers_training = _answers_to_rating(get_answers(config.get_man_answers_for_training_path()))

    diagrams = {}

    for batch_size in config.get_batch_sizes():
        for id in batch_size.ids:
            log.debug(f"Calculate graph for batch size: {batch_size.size} and id: {id}")
            bert_ai_model_ai_training = get_ratings(config.get_rated_answers_path('bert_ai', 'ai-training', batch_size.size, id), questions_path)
            bert_ai_model_ai_rating = get_ratings(config.get_rated_answers_path('bert_ai', 'ai-rating', batch_size.size, id), questions_path)
            bert_ai_model_man_rating = get_ratings(config.get_rated_answers_path('bert_ai', 'man-rating', batch_size.size, id), questions_path)

            bert_man_model_man_training = get_ratings(config.get_rated_answers_path('bert_man', 'man-training', batch_size.size, id), questions_path)
            bert_man_model_man_rating = get_ratings(config.get_rated_answers_path('bert_man', 'man-rating', batch_size.size, id), questions_path)
            bert_man_model_ai_rating = get_ratings(config.get_rated_answers_path('bert_man', 'ai-rating', batch_size.size, id), questions_path)

            xgb_ai_model_ai_training = get_ratings(config.get_rated_answers_path('xgb_ai', 'ai-training', batch_size.size, id), questions_path)
            xgb_ai_model_ai_rating = get_ratings(config.get_rated_answers_path('xgb_ai', 'ai-rating', batch_size.size, id), questions_path)
            xgb_ai_model_man_rating = get_ratings(config.get_rated_answers_path('xgb_ai', 'man-rating', batch_size.size, id), questions_path)

            xgb_man_model_man_training = get_ratings(config.get_rated_answers_path('xgb_man', 'man-training', batch_size.size, id), questions_path)
            xgb_man_model_man_rating = get_ratings(config.get_rated_answers_path('xgb_man', 'man-rating', batch_size.size, id), questions_path)
            xgb_man_model_ai_rating = get_ratings(config.get_rated_answers_path('xgb_man', 'ai-rating', batch_size.size, id), questions_path)

            # bert
            _get_figure_for_dataset(diagrams, id, 'bert_ai', 'ai-training').plot(batch_size.size, *_calculate_kappa_score(bert_ai_model_ai_training, ai_answers_training))
            _get_figure_for_dataset(diagrams, id, 'bert_ai', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(bert_ai_model_ai_rating, ai_answers_rating))
            _get_figure_for_dataset(diagrams, id, 'bert_ai', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(bert_ai_model_man_rating, man_answers_rating))
            
            _get_figure_for_dataset(diagrams, id, 'bert_man', 'man-training').plot(batch_size.size, *_calculate_kappa_score(bert_man_model_man_training, man_answers_training))
            _get_figure_for_dataset(diagrams, id, 'bert_man', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(bert_man_model_man_rating, man_answers_rating))
            _get_figure_for_dataset(diagrams, id, 'bert_man', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(bert_man_model_ai_rating, ai_answers_rating))

            _get_figure_for_dataset(diagrams, id, 'bert_ai-v-man', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(bert_ai_model_ai_rating, bert_man_model_ai_rating))
            _get_figure_for_dataset(diagrams, id, 'bert_ai-v-man', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(bert_ai_model_man_rating, bert_man_model_man_rating))

            # xgb
            _get_figure_for_dataset(diagrams, id, 'xgb_ai', 'ai-training').plot(batch_size.size, *_calculate_kappa_score(xgb_ai_model_ai_training, ai_answers_training))
            _get_figure_for_dataset(diagrams, id, 'xgb_ai', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_ai_model_ai_rating, ai_answers_rating))
            _get_figure_for_dataset(diagrams, id, 'xgb_ai', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_ai_model_man_rating, man_answers_rating))
            
            _get_figure_for_dataset(diagrams, id, 'xgb_man', 'man-training').plot(batch_size.size, *_calculate_kappa_score(xgb_man_model_man_training, man_answers_training))
            _get_figure_for_dataset(diagrams, id, 'xgb_man', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_man_model_man_rating, man_answers_rating))
            _get_figure_for_dataset(diagrams, id, 'xgb_man', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_man_model_ai_rating, ai_answers_rating))

            _get_figure_for_dataset(diagrams, id, 'xgb_ai-v-man', 'man-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_ai_model_ai_rating, xgb_man_model_ai_rating))
            _get_figure_for_dataset(diagrams, id, 'xgb_ai-v-man', 'ai-rating').plot(batch_size.size, *_calculate_kappa_score(xgb_ai_model_man_rating, xgb_man_model_man_rating))

    # Thats ugly
    score_1_v_2 = KappaFigure("Kappa of score_1 vs. score_2", "score_1_vs_score_2")
    plt.xlabel('Source')
    score_1_v_2.plot("AI-Dataset", _calculate_kappa(_get_scores_1(all_ai_answers), _get_scores_2(all_ai_answers)), -1)
    score_1_v_2.plot("Expert-Dataset", _calculate_kappa(_get_scores_1(all_man_answers), _get_scores_2(all_man_answers)), -1)
    score_1_v_2.save(config, f"The AI-Dataset consists of {len(all_ai_answers)} samples.\n The Expert-Dataset consists of {len(all_man_answers)} samples.")

    for id, diagram in diagrams.items():
        diagram.save(config)